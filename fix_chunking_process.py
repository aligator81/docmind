#!/usr/bin/env python3
"""
Fix the chunking process by updating the database and testing with existing files
"""

import os
import sys
import asyncio
from pathlib import Path

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent / "backend"))

from app.database import SessionLocal
from app.models import Document
from app.services.document_processor import DocumentProcessor
from app.services.document_chunker import DocumentChunker
from app.services.optimized_embedding_service import OptimizedEmbeddingService

async def fix_database_and_test():
    """Fix database issues and test the complete processing pipeline"""
    print("üîß Fixing Chunking Process")
    print("=" * 60)
    
    db = SessionLocal()
    try:
        # Check current state
        documents = db.query(Document).all()
        print(f"üìÑ Current documents in database: {len(documents)}")
        
        # Check what files actually exist
        uploads_dir = Path("data/uploads")
        existing_files = list(uploads_dir.glob("*"))
        print(f"üìÅ Existing files in uploads: {len(existing_files)}")
        
        for file in existing_files:
            print(f"  - {file.name} ({file.stat().st_size} bytes)")
        
        # Fix the failed document or create a new one
        failed_docs = db.query(Document).filter(Document.status == "failed").all()
        
        if failed_docs:
            print(f"\nüîÑ Fixing {len(failed_docs)} failed documents...")
            for doc in failed_docs:
                # Check if we have a matching file
                matching_file = None
                for file in existing_files:
                    if file.name in doc.filename or doc.filename in file.name:
                        matching_file = file
                        break
                
                if matching_file:
                    print(f"üìÑ Found matching file for document {doc.id}: {matching_file.name}")
                    # Update document with correct file path
                    doc.file_path = str(matching_file)
                    doc.filename = matching_file.name
                    doc.original_filename = matching_file.name
                    doc.file_size = matching_file.stat().st_size
                    doc.status = "not processed"
                    print(f"‚úÖ Updated document {doc.id} with correct file path")
                else:
                    print(f"‚ùå No matching file found for document {doc.id}, deleting...")
                    db.delete(doc)
        
        # If no documents exist or all were deleted, create a new one
        current_docs = db.query(Document).count()
        if current_docs == 0 and existing_files:
            print(f"\nüìÑ Creating new document from existing file: {existing_files[0].name}")
            new_doc = Document(
                filename=existing_files[0].name,
                original_filename=existing_files[0].name,
                file_path=str(existing_files[0]),
                file_size=existing_files[0].stat().st_size,
                mime_type="text/markdown" if existing_files[0].suffix == ".md" else "application/octet-stream",
                user_id=1,  # Assuming user ID 1 exists
                status="not processed"
            )
            db.add(new_doc)
        
        db.commit()
        
        # Now test the complete processing pipeline
        print("\nüîÑ Testing Complete Processing Pipeline")
        print("=" * 40)
        
        # Get the document to process
        doc_to_process = db.query(Document).filter(
            Document.status == "not processed"
        ).first()
        
        if not doc_to_process:
            print("‚ùå No documents to process")
            return
        
        print(f"üìÑ Processing document: {doc_to_process.filename} (ID: {doc_to_process.id})")
        
        # Step 1: Extract content
        print("\n1Ô∏è‚É£ Step 1: Document Extraction")
        processor = DocumentProcessor()
        extraction_result = await processor.extract_document(
            doc_to_process.file_path,
            original_filename=doc_to_process.original_filename
        )
        
        if not extraction_result.success:
            print(f"‚ùå Extraction failed: {extraction_result.method}")
            doc_to_process.status = "failed"
            db.commit()
            return
        
        print(f"‚úÖ Extraction successful using {extraction_result.method}")
        print(f"üìÑ Extracted {len(extraction_result.content)} characters")
        
        # Update document
        doc_to_process.content = extraction_result.content
        doc_to_process.status = "extracted"
        db.commit()
        
        # Step 2: Chunk the document
        print("\n2Ô∏è‚É£ Step 2: Document Chunking")
        chunker = DocumentChunker()
        chunking_result = await chunker.process_document_from_db(db, doc_to_process.id)
        
        if not chunking_result.success:
            print(f"‚ùå Chunking failed: {chunking_result.metadata}")
            doc_to_process.status = "failed"
            db.commit()
            return
        
        print(f"‚úÖ Chunking successful - created {chunking_result.chunks_created} chunks")
        print(f"üìä Metadata: {chunking_result.chunks_created} chunks with enhanced metadata")
        
        # Step 3: Create embeddings
        print("\n3Ô∏è‚É£ Step 3: Embedding Creation")
        embedding_service = OptimizedEmbeddingService()
        embedding_result = await embedding_service.process_embeddings_from_db(db)
        
        if not embedding_result.success:
            print(f"‚ùå Embedding creation failed: {embedding_result.metadata}")
            # Don't mark as failed since chunking worked
            doc_to_process.status = "chunked"
        else:
            print(f"‚úÖ Embedding creation successful - created {embedding_result.embeddings_created} embeddings")
            doc_to_process.status = "processed"
        
        db.commit()
        
        print(f"\nüéâ Processing pipeline completed successfully!")
        print(f"üìä Final status: {doc_to_process.status}")
        
    except Exception as e:
        print(f"‚ùå Error fixing chunking process: {e}")
        import traceback
        traceback.print_exc()
        db.rollback()
    finally:
        db.close()

async def main():
    """Main function to fix and test the chunking process"""
    print("üîß Docling Chunking Process Fix")
    print("=" * 60)
    
    await fix_database_and_test()
    
    print("\n" + "=" * 60)
    print("üìã FIX SUMMARY")
    print("=" * 60)
    print("The chunking process has been fixed and tested.")
    print("Next steps:")
    print("1. Upload new documents through the web interface")
    print("2. Use the processing endpoints to extract, chunk, and embed")
    print("3. Monitor the backend logs for any issues")

if __name__ == "__main__":
    asyncio.run(main())